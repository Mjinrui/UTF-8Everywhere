<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
	<title>无处不在的 UTF-8</title>
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
	<link rel="stylesheet" type="text/css" href="data/style.css"/>
	<meta name="viewport" content="width=device-width, initial-scale=1"/>
	<script type="text/javascript" src="https://apis.google.com/js/plusone.js"></script>
</head>
<body>
<div class="topDiv">
<h1><a class="aId" href="#">无处不在的 UTF-8</a></h1>
<p class="subtitle">宣言</p>

<h2 id="intro"><a class="aId" href="#intro">这份文档的目的</a></h2>

<aside class="rightFloater">这份文档包含特殊字符。如果没有合适的渲染支持，你也许会看到问号，方块或其它符号。</aside>


<p>我们的目标是促进 UTF-8 编码的使用与支持，让人们确信 UTF-8 编码应该是在内存或磁盘中存储文本字符串，用于沟通和所有其他用途的默认编码选择。我们相信我们的方案提升了性能，降低了软件的复杂度并且避免了很多 Unicode 相关的错误。我们相信所有其它 Unicode（或文本）的编码适用于优化极少的边界情况，主流用户应该避免使用。</p>

<p>特别地，我们认为非常流行的 UTF-16 编码（在 Windows 世界被错用做宽字符（widechar）和 Unicode 的同义词）不应该用在库 APIs 中，除非用于处理文本的特殊的库如 ICU。</p>

<p>本文档建议在 Windows 应用程序中也使用 UTF-8 编码存储字符串，尽管由于历史原因和原生API级别 UTF-8 支持的缺乏这个标准在这个领域并不流行。我们相信即使在这个平台上，后面的一些观点胜过原生支持的缺乏。并且，我们建议永远忘记内码表（ANSI codepages）和它们的用途。在文本字符串中混用任意种语言是属于用户的权利。</p>

<p>在工业上，很多本地化相关的 bugs <a href="http://www.joelonsoftware.com/articles/Unicode.html">源于</a>程序员缺乏 Unicode 编码知识。然而，我们认为，如果一个应用不是专业用于文本，它的架构应该能够让程序无需考虑编码问题。举例来说，一个文件复制工具不应该为了支持非英语文件名而写的不同。在这份宣言里，我们还将告诉一个程序员如果他不想陷入 Unicode 的复杂之中并且不关心 <a href="#cookie">字符串的内部</a>，他应该怎么做。</p>

<p>此外，在文本处理情景中，我们建议不应该将计数或者遍历 Unicode 码点视为特别重要的任务。许多开发者错误地将码点视为 ASCII 字符的继承。这导致了像 Python 字符串 O(1) 码点获取。然而，事实上 Unicode 非常复杂并且没有像 <em>Unicode 字符</em>这样的通用定义。相对与 Uncoide 字符族，码元或者甚至语言中的单词，我们没有特殊的原因去更青睐 Unicode 码点。另一方面，将 UTF-8 码元视为一个文本的基本单元对于很多任务来说特别有用，如解析常用的字符数据类型。这源于这种编码的特性。字位，码元，码点和其他相关的 Unicode 相关的术语在 <a href="#characters">部分 5 </a>有解释。编码字符串的操作在 <a href="#textops">部分 7</a> 有讨论。</p>

<h2 id="background"><a class="aId" href="#background">背景</a></h2>

<p>1988年，Joseph D. Becker 发布了<a href="http://unicode.org/history/unicode88.pdf">第一个 Unicode 草案提议</a>。他的设计的根据是一个天真的假设:每个字符 16 位就足够。在1991年，Unicode 标准的第一个版本发布，码点被限制在16位。在接下来的几年中，许多系统添加了对 Unicode 的支持并且转换到了 UCS-2 编码。对于像Qt框架，(1992)，Windows NT 3.1(1993)和 Java (1995)等当时的新技术，它是非常有吸引力的。</p>

<p>然而，很快就发现每个字符16位对于 Unicode 是不够的。在1996年，UTF-16编码被创造出来，从而使现存的系统能够处理非16位的字符。这使得最初选用16位字符编码，也就是固定宽度编码的理论没有了依据。现在，Unicode 包含超过109449个字符，其中大约74500个属于中日韩（CJK）统一表意文字。</p>

<div style="text-align:center">
	<img src="data/nagoya-museum.jpg" style="text-align: center; max-width: 100%; width: 480px;" alt="A little child playing an encodings game in front of a large poster about encodings."/>
	<div style="font-style: italic; margin:1ex;">名古屋科学博物馆。Vadim Zlotnik 摄。</div>
</div>

<p>微软经常错误地将 Unicode 和宽字符 (widechar) 作为了 UCS-2 和 UTF-16 的同义词。而且，因为 UTF-8 不能被设置为窄字符串的 WinAPI 的编码，人们必须定义 <code>UNICODE</code> 编译代码。Windows C++ 程序员被教育 Unicode 必须和宽字符（甚至是使用与编译器设置相关的TCHARs，允许程序员不支持全部的 Unicode 码点）一起使用。作为这些混乱的结果，许多 Windows 程序员在关于如何正确处理文本非常疑惑。</p>

<p>与此同时，在 Linux 和 Web 世界里，有一个默认的共识：UTF-8 是地球上对 Unicode 最好的编码。尽管相对于其它的文本，它对英语，包括计算机语言（例如 C++，HTML，XML等）提供了更短的表示，在常用的字符集上，它也很少比 UTF-16 编码效率低。</p>

<h2 id="facts"><a class="aId" href="#facts">事实</a></h2>

<ul>
	<li>在 UTF-8 和 UTF-16 编码中，码点都是最多占据 4 个字节。</li>

	<li>UTF-8 是端性中立的。UTF-16 有两种格式：UTF-16LE 和 UTF-16BE（分别对应不同的字节序）。这里我们将他们统称为UTF-16。</li>

	<li>宽字符（widechar）在一些平台上2个字节大小，在其他平台上4个字节。</li>

	<li>当按字典序排序的时候，UTF-8 和 UTF-32 顺序相同，UTF-16则不同。</li>

	<li>UTF-8 编码处理在英语字母和其它 ASCII 字符上效率更高（每个字符一个字节），而 UTF-16 编码则在处理几个亚洲字符集上效率更好（每个字符两个字节而不是 UTF-8编码中的3个）。在 Web 世界中，英语的 HTML/XML 标签和各种文本混在一起，这使得 UTF-8 成为了最受欢迎的选择。西里尔字母，希伯来语和几个其它流行的 Unicode 字符表块在 UTF-16 和 UTF-8 中都是 2 个字节。</li>

	<li>UTF-16 经常被误用做固定宽度的编码，即使在 Windows 原生应用中：在普通文本编辑控制中（直到 Vista）,删除一个在 UTF-16 中占据 4 个字节的字符，要使用两个退格键。在 Windows 7中，控制台将该字符显示为两个无效字符，无论使用什么字体。</li>

	<li>许多 Windows 第三方库不支持 Unicode：它们使用窄字符串参数并且将它们传递给 ANSI API。即使文件名也是这样。总的来说，这样是行不通的，因为字符串不可能使用一个 ANSI 页码表示完全（如果它包含来自不同 Unicode 区块的字符）。在 Windows 上解决文件名问题的方案是添加一个 8.3 路径到这个文件上并添加到该类库上。如果这个库需要建立一个不存在的文件，这将不可行。如果路径非常长并且8.3格式比 <code>MAX_PATH<code> 长，这也将不可行。如果在操作系统设置中短名生成不可行，这仍将不可行。</li>

	<li>在C++中，除了使用 UTF-8 编码，没有方法能够从 <code>std::exception::what()</code> 返回 Unicode。除了使用 UTF-8，没有别的办法使 Unicode 支持 <code>localeconv</code>。</li>

	<li>UTF-16 在当今仍然流行，即使在 Windows 世界之外。Qt，Java，C#，Python（在 CPython 3.3版本参考实现之前，<a href="#faq.python">参考</a>）和 <a href="http://en.wikipedia.org/wiki/International_Components_for_Unicode"> ICU </a> —— 他们都是用 UTF-16 作为内部字符串表示。</li>
</ul>

<h2 id="cookie"><a class="aId" href="#cookie">透明数据争论</a></h2>

<p>让我们回到文件复制工具。在 UNIX 世界中，窄字符串几乎到处被默认为 UTF-8 编码。因此，文件复制工具的作者不需要考虑 Unicode。一旦 ASCII 字符串作为文件名参数测试通过，任何其他语言的文件名都会工作正常，as arguments are treated as  <a href="http://en.wikipedia.org/wiki/Opaque_data_type">cookies</a>. 复制工具的代码<em>一点</em>也不需要改变就可以支持其他语言。<code>fopen()</code> 无缝的接收 Unicode 字符，<code>argv</code> 也同样如此。</p>

<p>现在，让我们看一看在基于 UTF-16 架构的微软 Windows 上如何实现。在这上面制作一个能够接受几种不同的 Unicode 块字符文件名参数的文件复制工具需要高深的技巧。首先，这个应用必须编译为 Unicode 感知的。在这个例子中，它不能有带有标准C参数的 <code>main()</code> 函数。它将接受 UTF-16 编码的 argv。为了使一个核心部分使用用窄字符文本的 Windows 应用程序接收 Unicode 字符，它必须深层次的重构并且认真处理每一个字符串变量。</p>

<p>和 MSVC 一同发布的标准库没有很好的支持 Unicode。它直接将窄字符串转发到系统的 ANSI API。没有办法重写这些。改变 <code>std::locale</code> 也无效。在 MSVC 上使用 C++ 的标准特性无法打开一个使用 Unicode 字符名称的文件。打开一个文件的标准方法是：<p>

<pre><code>std::fstream fout(&quot;abc.txt&quot;);</code></pre>

<p>解决这个问题的方式是使用微软自身接受宽字符参数的hack，但这不是标准的扩展。</p>

<p>在 Windows 上，<code>HKLM\SYSTEM\CurrentControlSet\Control\Nls\CodePage\ACP</code> 注册表键值能够使接受非 ASCII 字符，但是这些字符只能来自单个 ANSI 字符页。在 Windows 上，一个未被实现的值 65001 将能够解决 cookie 问题。 如果微软实现对这个 <abbr title="ANSI Code Page">ACP</abbr> 值的支持，这将有助于 UTF-8 编码在 Windows 平台上更广泛的应用。</p>

<p>对于 Windows 程序员和多平台库的开发商，我们会在 <a href="#windows">Windows平台如何处理文本</a> 部分深入探讨我们为了更好支持Unicode 的处理文本字符串和重构程序的方法。</p>


<h2 id="characters"><a class="aId" href="#characters">Glyphs, graphemes 和其它 Unicode 类型</a></h2>
    <p>这里有一个带有我们评论的按照 Unicode 标准的关于字符，码点，码元和象形聚簇定义的摘录。你可以参考标准的相关部分获取更多详细的描述。</p>

	<aside class="rightFloater"><p class="display" style="font-size:1.5em">Приве́т नमस्ते שָׁלוֹם</p><p>你看到了多少<em>字符</em>？</p></aside>

	<dl>
		<dt>码点(code point)</dt>
		<dd>Unicode 编码空间的任何数值。<sup>[§3.4, D10]</sup> 举例: U+3243F. </dd>

		<dt>码元(code unit)</dt>
        <dd>能够代表一个编码文本单元的最小位组合。<sup>[§3.9, D77]</sup> 举例，UTF-8，UTF-16 和 UTF-32 分别使用 8位，16位和 32 位的码元。上面提到的码点将 会被编码为 ‘<code>f0 b2 90 bf</code>’使用UTF-8，'<code>d889 dc3f</code>' 使用 UTF-16， '<code>0003243f</code>' 使用 UTF-32。注意这些仅仅是<em>位组(groups of bits)</em>的次序；它们如何 被存储进一步取决于相关编码的端性。所以，当存储上面的 UTF-16 码元在一个 octet 的媒介上，他们将会被转化为 '<code>d8 89 dc 3f</code>' 在 UTF-16BE 编码中，在 UTF-16LE中则是 '<code>89 d8 3f dc</code>' 。<dd>

		<dt>抽象字符(Abstract character)</dt>
		<dd>
			<p>用来组织，控制和表示文本数据的一组信息单元。<sup>[§3.4, D7]</sup> 在 §3.1 中标准进一步说明：</p>

			<blockquote><p>对于 Unicode 标准，[...] 这个字符集本质上是开放的。因为Unicode是一个通用的编码，任何被编码过的抽象字符都将是被编码的候选，不管这个字符现在是否为人们所知。</p></blockquote>

			<p>这个定义真的很抽象。任何人们能够想到可以<em>作为</em>字符都是一个抽象字符。举个例子 <img src="data/glyph-ungwe.png" style="vertical-align: -1ex" alt=""/> <em>tengwar 字母 ungwe</em>是一个抽象字符，虽然它还不能在 Unicode 中表示。</p>

		</dd>

		<dt>编码字符(Encoded character)</dt>
		<dt>Coded character</dt>
		<dd>
			<p>一个码点和一个抽象字符的映射。<sup>[§3.4, D11]</sup> 举例，U+1F428 是一个已编码字符，代表抽象字符 🐨 <span class="uniname">考拉</span>.</p>

			<p>这种映射既不是完整的，也不是单射，也不是满射：</p>

			<ul>
				<li>代表，非字符和未分配的码点不对应抽象字符。</li>
				<li>一些抽象字符可以使用不同的码点编码； U+03A9 <span class="uniname">希腊大写字母 omega</span> 和 U+2126 <span class="uniname">欧姆符号</span> 都对应相同的抽象字符 'π' ，并且<em>必须被同样的看待</em>。</li>
				<li>一些抽象字符不能被一个单一码点编码。它们用已编码字符<em>序列</em>表示。举例来说，表示抽象字符 ю́ <em>cyrillic small letter yu with acute</em> 的唯一方法是用序列 U+044E <span class="uniname"></span> 紧跟 U+0301 <span class="uniname">combining acute accent</span>。</li>
			</ul>

			<p> 还有，对于一些抽象字符，除了使用单码点的已编码字符的形式，<em>还</em>存在着使用多码点的表示方法。抽象字符 ǵ 可以用单一码点 U+01F5 <span class="uniname">latin small letter g with acute</span>编码，或者被 &lt;U+0067 <span class="uniname">latin small letter g</span>, U+0301 <span class="uniname">combining acute accent</span>&gt 序列编码。</p>
		</dd>

		<dt>用户感知字符(User-perceived character)</dt>
		<dd>任何终端用户看为一个字符的字符。这个概念是依赖于语言的。举例来说，'ch' 在英语和拉丁语中是两个字母，但是 在捷克语和 Slovak 中被看做一个字母。</dd>

		<dt>字形族(Grapheme cluster)</dt>
		<dd>一个应该保持在一起的已编码字符序列。<sup>[§2.11]</sup>字形族在单独语言的角度看接近用户感知字符。他们被 用来做光标移动和选择。</dd>
	</dl>

	<p>字符可能意味着上面的任意定义。Unicode 标准将它作为<em>已编码字符</em>的同义词[§3.4]。当一些编程语言或者库文件说到‘字符’，这总是意味着一个码元。当一个终端用户被问到一个字符串中字符的数量的时候，她将计数用户察觉的字符。根据一个程序员的的 Unicode 熟悉程度，他会将码元，码点或 grapheme 聚簇数做字符。例如， <a href="https://dev.twitter.com/docs/counting-characters">推特如何计算字符数量</a>。我们认为，对字符串 '🐨’'，一个求字符串长度函数不一样一定返回 1 才是兼容 Unicode 的。</p>

<h2 id="asian"><a class="aId" href="#asian">亚洲文字: UTF-8 vs. UTF-16</a></h2>
	<p>大多数的 Unicode 码点使用 UTF-8 和 UTF-16 编码占据相同的字节数。这包括俄罗斯文，希伯来文，希腊文。并且所有的 non-BMP （非基本面）码点在两种编码中都占有2个或4个字节。尽管一些亚洲字符使用 UTF-8 占用更多空间，使用 UTF-16 编码会在拉丁字母，标点符号和其余的 ASCII 字符上占用更多的空间。亚洲程序员们怎么能够不反对放弃每个字符节省他们 50% 内存的的 UTF-16编码呢？</p>

	<p>事实上，只有在人为构建的仅包含 U+0800 到 U+FFFF 范围内字符的例子中才是这样。然而，电脑和电脑间的文本接口比文本的其他应用要广泛。这包含 XML，HTTP，系统路径和配置文件——他们几乎都使用独家的 ASCII 字符，并且事实上在相关的亚洲国家 UTF-8 使用的同样多。</p>

	<p>对于中文书的专门存储，UTF-16 依旧可以被使用来作为不错的优化。一旦文本从这样的存储中取出来，它应该被转换为和世界相兼容的标准。在两种情况下，如果存储是昂贵的，无损压缩总是会被使用。在这样的情况下，UTF-8 和 UTF-16 将占据大致差不多的空间。进一步，“在语言中，一个象形文字比拉丁字符传递更多的信息，所以它占据更多的空间也很合理” (Tronic, <a href="http://programmers.stackexchange.com/a/102211/34925">UTF-16 considered harmful</a>)。</p>

	<p>这里有一个简单实验的结果。一些网页（日语文章，2012年1月1日采集于日语维基百科）的 HTML 源文件占据的空间在第一纵列。第二纵列显示的是去除标记后的这些文本的结果，就是选择全部，复制，粘贴到纯文本文件中。</p>
	<table class="basicTable" style="width:100%">
		<tbody>
			<tr><th></th><th>HTML Source (Δ UTF-8)</th><th>Dense text (Δ UTF-8)</th></tr>
			<tr><th>UTF-8</th><td>767 KB (0%)</td><td>222 KB (0%)</td></tr>
			<tr><th>UTF-16</th><td>1 186 KB (+55%)</td><td>176 KB (−21%)</td></tr>
			<tr><th>UTF-8 zipped</th><td>179 KB (−77%)</td><td>83 KB (−63%)</td></tr>
			<tr><th>UTF-16LE zipped</th><td>192 KB (−75%)</td><td>76 KB (−66%)</td></tr>
			<tr><th>UTF-16BE zipped</th><td>194 KB (−75%)</td><td>77 KB (−65%)</td></tr>
		</tbody>
	</table>

	<p>可以看出，UTF-16 在实际数据上占据的空间比 UTF-8 多 50%，对于纯亚洲文字仅仅节约 20% 的空间并且在用通用压缩算法处理后很难有竞争力。</p>

<h2 id="textops"><a class="aId" href="#textops">编码字符串的文本操作</a></h2>
	<p>流行的基于文本的数据格式（如CSV, XML, HTML, JSON, RTF和计算机程序的源码）经常包含 ASCII 字符作为结构控制元素并且包含ASCII和非ASCII数据字符串。处理ASCII字符码点长度比其他码点长度短的变长编码似乎是一个困难的任务，因为字符串内的编码字符的界限不是立即可知的。这使得软件架构选择 UCS-4 定长编码(例如，<a href="#faq.python">Python v3.3</a>)。事实上，这既是不必要的，也并没有解决我们知道的任何实际问题。</p>

	<p>自UTF-8设计之初，UTF-8 保证一个 ASCII 字符值或者子串永远不会匹配到一个多字节编码字符的一部分。UTF-16也是这样。在两种编码中，多部分编码码点的码元会将<abbr title="Most Significant Bit">MSB</abbr> 设置为1。</p>

	<p>‘&lt;’ 符号标志着一个 HTML 标签的开始，或者一个UTF-8 编码的 SQL 语句中的（`）防止 SQL 注入，正如你讲作为一个全英语的文本 ASCII 字符串。编码保证这个能够运行。特别地，每一个非 ASCII 字符都作为字节序列编码为UTF-8的，序列中的每一个都有一个大于 127 的值。这使得原有的算法不可能发生碰撞 --- 简单，快速和优雅，并且不需要在意已编码字符边界。 </p>

	<p> 并且，在从一个 UTF-8 编码字符串中查找一个非ASCII，UTF-8 编码的子串时你可以将它看做一个纯字节数组，不需要关心码点的边界。这归功于 UTF-8 编码的另一个设计特性----一个码点前面的字节永远不能与任何其他码点的后面的字节之一有相同的值。</p>

<h2 id="myths"><a class="aId" href="#myths">计算字符上的更深的谜题</a></h2>

<p>正如我们已经说明的，在一个 Unicode 字符串中计数，分割，索引和其他遍历码点的操作应该被视为经常和重要的操作。在这部分，我们将认真地重看这部分。</p>

<h3 id="myth.utf16.o1"><a class="aId" href="#myth.utf16.o1">1. 使用 UTF-16，计算字符数可以在一个常量时间内完成。</a></h3>

<p>这是那些认为 UTF-16 是一个固定宽度编码的人的一个共有错误。不是这样的，事实上 UTF-16 是做一个变长编码。如果你依然否认非BMP 字符的存在，参考<a href="#faq.almostfw">这个常见问题集</a>。</p>

<h3 id="myth.utf32.o1"><a class="aId" href="#myth.utf32.o1">2. 使用 UTF-32，计算字符数量可以在一个常量时间内完成。</a></h3>

<p>这取决于被误用的词语 ‘字符’ 的含义。在 UTF-32 中，我们是可以在常数时间内计数码元和码点。然而，码点 并不一定对应用户可觉的字符。即使在 Unicode 形式中一些码点对应<em>编码字符</em>，一些对应<em>非字符</em>。</p>

<h3 id="myth.strlen"><a id="myth.nth.char"></a><a class="aId" href="#myth.strlen">3. 给已编码字符或者码点计数是重要的</a></h3>

<p>码点的重要性经常被夸大。这是由于对仅仅反映人类语言复杂性的 Unicode 复杂度的误解。很容易说出在 'Abracadabra' 中有多少个字符，但是对于下面的字符串却并不是那么简单：</p>

<p class="display">Приве́т नमस्ते שָׁלוֹם</p>

<p>上面的字符串包含 22(!) 码点，但仅仅有 16 个象形族。所以，‘Abracadabra’ 包含 11 个码点，上面的字符串包含 22 个码点，如果转换为<a href="http://unicode.org/reports/tr15/">NFC</a> 则进一步仅有 20 个码点。然而，码点的数量和几乎与任何软件工程问题无关，唯一的例外是将字符串转为 UTF-32。举个例子：</p>

<ul>
	<li>For cursor movement, text selection and alike, <em>grapheme clusters</em> shall be used. <!-- 5.11 --></li>
	<li>对于光标移动，文本选择，<em>象形族(grapheme clusters)</em> 将会被使用。</li>
	<li>对于在输入领域，文本格式，协议和数据库上限制字符串长度的情况，长度是根据提前指定的编码的<em>码元</em>测算出来的。原因是任何长度限制是来源于在低层次给字符串分配的固定量的内存，在内存中，磁盘中或者在一个特定的数据结构中。</li>
	<li>字符串在屏幕上显示的大小和字符串中码点的数量无关。为了解这些，不得不通过渲染引擎。码点不占据一列即使在终端和等宽字体中。POSIX 考虑到了这点。</li>
</ul>

<h3 id="myth.nfc"><a class="aId" href="#myth.nfc">4. 在 NFC 中，每个码点对应一个用户可查的字符</a></h3>

<p>不，因为可以在 Unicode 中表示的用户可见字符的数量是无限的。即使在实践中，大多数字符并没有一个完全组成的形式。举个例子，上面例子中的包含三个<em>真实</em>语言的三个<em>真实</em>词语的 NFD 字符串将在 NFC 中包含 20 个码点。这远比它包含的 16 个可见字符多。</p>

<h3 id="myth.strlen.correctness"><a class="aId" href="#myth.strlen.correctness">5. <code>length()</code> 字符串操作一定计算用户可见字符或者已编码字符的数量。如果不是，它就没能正确地支持 Unicode。</a></h3>

<p>程序库和编程语言对 Unicode 的支持经常通过返回字符串长度操作的值来判断。根据对 Unicode 支持的测试，大多数流行的语言，例如 C#,Java 甚至连 ICU 都不支持 Unicode。举个例子，一个字符串 ‘🐨’的长度在UTF-16 作为内部字符串表示的地方经常被报告为2，在内部使用 UTF-8 的语言中则是 4。误解的来源是这些语言的说明中使用字符表示一个码元，尽管程序员希望它是其它的东西。</p>

<p>因此，这些 API 返回的码元数实际上非常重要。当写一个 UTF-8 字符串到一个文件，最重要的是字节数的长度。计数任何类型的字符，并不是非常有用。</p>

<h2 id="conclusions"><a class="aId" href="#conclusions">我们的结论</a></h2>

<p>UTF-16 的两个方面都是最糟糕的，长度可变和太宽。它因为历史的原因而存在，造成了许多的迷惑。我们希望它的使用进一步减少。</p>

<p>可移植性，跨平台协作和简单性这些都比和现有平台的 APIs 相容更重要。所以，最好的方案是在 Windows 上每个地方都使用 UTF-8 窄字符串并且在调用接收字符串的 APIs 前将它们来回转换。当处理接收字符串的系统 APIs 时， <a href="#faq.cvt.perf">性能几乎不与此相关</a>，但是在各个地方使用相同的编码有巨大的好处，<a href="#faq.liberal">我们没有足够的理由不去这么做。</a></p>

<p>说到性能，机器经常使用字符串进行通信（如HTTTP头，XML）。许多人将此视为错误，虽然这几乎总是通过英文完成，在这个方面给了 UTF-8 更多的优势。不同的字符串使用不同的编码增加了复杂度和因此造成的 bugs。</p>

<p>尤其，我们认为给 C++ 增加 <code>wchar_t</code> 类型是一个错误，C++11添加的 Unicode 补充也一样。虽然这样，最需要实现的是<em>基本执行字符集</em>能够存储任何 Unicode 数据。然后，每个 <code>std::string</code> 或者 <code>char*</code> 参数都必须是 Unicode 兼容的。 ‘只要接受文本，就是应该是 Unicode 兼容的'——使用 UTF-8，这点很容易做到。</p>

<p><a href="http://www.boost.org/doc/libs/1_58_0/libs/locale/doc/html/rationale.html">这个标准的部分有许多设计瑕疵</a>。这包括 <code>std::numpunct</code>，<code>std::moneypunct</code> 和 <code>std::ctype</code> 都不支持变长编码字符（非ASCII的 UTF-8 和 非 BMP 的 UTF-16）。他们必须被修正：</p>

<ul>
	<li><code>decimal_point()</code>和<code>thousands_sep()</code>应该返回一个字符串而不是一个码元。这就是 C locales 通过<code>localeconv</code>函数支持这个，尽管并不可定制。</li>

	<li><code>toupper()</code>和<code>tolower()</code> 在和码元（code units）相关的时候也不应该被使用，因为它不支持在 Unicode 下工作。举例来说，拉丁连体字符 ffl 必须被转换成 FFL，德语 ß 必须被转化为 SS（有一个大写形式的ẞ，但是转化规则遵循传统的方式）。除此之外，一些语言（例如希腊语）有一些小写字母的最终特殊形式，这种情况下转换必须清楚正确地进行转换的位置。</li>
</ul>


<h2 id="windows"><a class="aId" href="#windows">在 Windows 上如何处理文本</a></h2>

<p>这部分是为了多平台库开发和 Windows 平台编程。Windows 平台的问题是它不支持与 Unicode 兼容的窄字符串系统 APIs。将 Unicode 字符串传给 Windows API 的唯一方式是通过转化成 UTF-16（即宽字符）。</p>

<p>注意我们的说明和微软的关于 Unicode 转换的原始说明显著不同。我们基于实施宽字符转换的方法尽可能与 API 调用相近，并且不处理宽字符数据。在上一部分我们解释了这将导致更好的性能，稳定性，代码简介和其他软件更好的集成。</p>

<ul>
	<li>不要在任何地方使用 <code>wchar_t</code>或<code>std::wstring</code>，除非在和接受 UTF-16 的APIs的临近点。</li>

	<li>不要在任何地方使用 <code>_T(&quot;&quot;)</code> 或 <code>L&quot;&quot;</code>修辞，除非用在接受 UTF-16 的APIs的参数上。</li>

	<li>不要使用对 <code>UNICODE</code> 常量敏感的类型，函数或者他们的衍生物，例如<code>LPTSTR</code>, <code>CreateWindow()</code>或<code>_T()</code>宏。相反，使用<code>LPWSTR</code>, <code>CreateWindowW()</code>和明确的<code>L""</code>字符量。</li>

    <li>为了避免传递给 ANSI WinAPI 的 UTF-8 字符串被悄悄地编译，<code>Unicode</code> 和 <code>_UNICODE</code> 仍总是要被定义。这个可以通过 VS 项目设置中的<em>使用 Unicode 字符集</em>设定</li>

    <li>程序中任何地点出现的 <code>std::string</code> 和 <code>char*</code> 都被看做 UTF-8 。</li>

    <li>如果你有幸写 C++，下面的 <code>narrow()</code>/<code>widen()</code> 转换函数对于 inline 转换语法非常方便。当然，任何其他 UTF-8/UTF-16 转换代码都可以。</li>

	<li>
		只使用接受宽字符（<code>LPWSTR</code>）的 Win32 函数，绝不使用那些接受 <code>LPTSTR</code> 或 <code>LPSTR</code> 的函数。用这种方法传递参数：
		<pre><code>::SetWindowTextW(widen(someStdString or &quot;string litteral&quot;).c_str())</code></pre>
		实践 (Policy) 使用下面描述的转换函数。参考，<a href="#faq.cvt.perf">一个关于转换效率的笔记</a>.
	</li>

	<li>
		对于 MFC 中的字符串：
		<pre><code>CString someoneElse; // something that arrived from MFC.

// Converted as soon as possible, before passing any further away from the API call:
std::string s = str(boost::format(&quot;Hello %s\n&quot;) % narrow(someoneElse));
AfxMessageBox(widen(s).c_str(), L&quot;Error&quot;, MB_OK);</code></pre>
	</li>

	<li>对于.NET开发者: 使用原始的基于 UTF-16 的字符串类也许很难避免。记住这个实现细节通过类的接口有重大缺陷。 举个例子，<code>string[index]</code> 操作也许会返回一个字符的部分（就像处理一个 UTF-8 字节序列）。当序列化字符串到文件或通讯设备的时候，记得明确 <code>Encoding.UTF8</code>。准备好为了转换付出效率代价。例如，在产生 UTF-8 HTML输出的 ASP.NET web应用中。</li>

</ul>

<h3 id="how.files"><a class="aId" href="#how.files">在 Windows 上处理文件，文件名和字符流</a></h3>

<ul>
	<li>总是生成 UTF-8 编码的文本输出文件。</li>

	<li>因为 <a href="http://en.wikipedia.org/wiki/RAII">RAII/OOD</a> 的原因，使用 <code>fopen()</code> 应该被避免。但是如果必要，按照上面描述的方式使用 <code>_wfopen()</code> 和 WinAPI。</li>

	<li>永远不要传递 <code>std::string</code> 和 <code>const char*</code> 文件名参数给 <code>fstream</code> 家族。MSVC CRT 不支持 UTF-8 参数，但是它有一个非标准的扩展应该按下面的方式使用。</li>

	<li>
		使用 <code>widen</code> 将 <code>std::string</code> 参数转化为 <code>std::wstring</code>：
		<pre><code>std::ifstream ifs(widen(&quot;hello&quot;), std::ios_base::binary);</code></pre>
		当 MSVC 对于 <code>fstream</code> 的态度变化的时候，我们将不得不手动去除该转换。
	<li>该代码不是多平台的并且在将来也许会不得不手动改变。</li>
	<li>也可以选择使用一组封装去隐藏转化。</li>
</ul>

<h3 id="how.cvt"><a class="aId" href="#how.cvt">转化函数</a></h3>

<p>这份指南使用的转化函数来自 <a href="http://cppcms.com/files/nowide/html/">Boost.Nowide library</a>(还不是 boost 的一部分)：</p>

<pre><code>std::string narrow(const wchar_t *s);
std::wstring widen(const char *s);
std::string narrow(const std::wstring &amp;s);
std::wstring widen(const std::string &amp;s);</code></pre>

<p>这个库也为常用的处理文件和使用 iostreams 读写 UTF-8 的C 和 C++ 库函数提供了一组封装。</p>

<p>这些函数和封装通过 Windows 的 <code>MultiByteToWideChar</code> 和 <code>WideCharToMultiByte</code> 函数很容易实现。任何其它 （也许更快的）转化方法也可以使用。</p>


<h2 id="faq"><a class="aId" href="#faq">常见问题</a></h2>

<ol class="faqList">
	<li>
		<h3 id="faq.linuxer"><a class="aId" href="#faq.linuxer">问：你是一个 Linux 使用者吗？这是一场隐藏的反对 Windows 的宗教战争吗？</a></h3>
		<p>答：不，我使用 Windows 长大，并且我基本上是一个 Windows 开发者。我认为他们在文本方面做出了一个错误的选择，因为他们比其他人做的都早。 —— Pavel</p>
	</li>

	<li>
		<h3 id="faq.anglophile"><a id="faq.angle.saxon"></a><a class="aId" href="#faq.anglophile">问：你是一个亲英派码？你是否私下认为英语字母和文化比其它要优越？</a></h3>
		<p>答：不，并且我的国家也并不是讲 ASCII 上字符的语言的国家。我不认为使用一个字节编码 ASCII 字符的编码是盎格鲁中心主义思想，或者与人们沟通有任何的关系。即使一个人可以争辩说程序，网页和 XML 文件，操作系统文件名和其它电脑和电脑的接口本不应该存在，只要他们存在，文本就不仅仅是给人类读者的。</p>
	</li>

	<li>
		<h3 id="faq.why.care"><a class="aId" href="#faq.why.care">问：你们这些人为什么在意？我使用 C# 和/或 Java 编程，我一点也不在意编码。</a></h3>

		<p>答：不对。值得庆祝的是，C# 和 Java 都提供 16 位，小于 Unicode 字符集的 <code>char</code> 类型。.NET 索引 <code>str[i]</code> 工作在内部表示单元，因此又出现一个有漏洞的抽象。子串方法将会返回一个无效的字符串，将一个非 BMP 字符分成几部分。</p>

		<p>此外，当你将文本写入到磁盘，网络通信，外部设备或其他任何程序会读取地方上的文件的时候，你不得不在意编码。 不要管任何对内容的假定，在这些情况一定要使用 <code>System.Text.Encoding.UTF8<code>(.Net) ，不要使用 <code>Encoding.ASCII<code>，UTF-16 或者手机PDU。</p>

		<p>Web 框架像 ASP.NET 也受制于框架之下内部字符串表示选择的匮乏：一个网络应用预期的字符串输出（或者输入）几乎总是 UTF-8 编码，导致在高输入输出的网络应用和网络服务中面临大量的转换。</p>
	</li>

	<li>
		<h3 id="faq.utf8.fossil"><a class="aId" href="#faq.utf8.fossil">问：UTF-8 不就是一个尝试兼容 ASCII 的一个尝试吗？为什么要保持这个老化石?</a></h3>
		<p>答：不管 UTF-8 是不是作为一个兼容的 hack 被发明出来。现在，它是一个比其它任何编码都优秀和流行的编码。</p>
	</li>

	<li>
		<h3 id="faq.almostfw"><a class="aId" href="#faq.almostfw">Q: UTF-16 characters that take more than two bytes are extremely rare in the real world. This practically makes UTF-16 a fixed-width encoding, giving it a whole bunch of advantages. Can’t we just neglect these characters?</a></h3>
		<p>A: Are you serious about not supporting all of Unicode in your software design? And, if you are going to support it anyway, how does the fact that non-BMP characters are rare practically change anything, except for making software testing harder? What does matter, however, is that text manipulations are relatively rare in real applications—compared to just passing strings around as-is. This means the "almost fixed width" has little performance advantage (see Performance), while having shorter strings may be significant.</p>
	</li>

	<li>
		<h3 id="faq.liberal"><a class="aId" href="#faq.liberal">Q: Why not just let any programmer use their favorite encoding internally, as long as they knows how to use it?</a></h3>
		<p>A: We have nothing against correct usage of any encoding. However, it becomes a problem when the same type, such as <code>std::string</code>, means different things in different contexts. While it is ‘ANSI codepage’ for some, for others, it means ‘this code is broken and does not support non-English text’. In our programs, it means Unicode-aware UTF-8 string. This diversity is a source of many bugs and much misery. This additional complexity is something the world does not really need. The result is lots of Unicode-broken software, industry-wide. JoelOnSoftware <a href="http://www.joelonsoftware.com/articles/Unicode.html" rel="nofollow">suggests</a> that having every programmer be aware of encodings is the solution to Unicode-broken software. We believe that with one mainstream encoding becoming the default for software APIs, one will be able to write a correct file copy program <a href="#cookie">without being an expert in text and language issues</a>.</p>
	</li>

	<li>
		<h3 id="faq.win.liberal"><a class="aId" href="#faq.win.liberal">Q: My application is GUI-only. It does not do IP communications or file IO. Why should I convert strings back and forth all the time for Windows API calls, instead of simply using wide state variables?</a></h3>
		<p>This is a valid shortcut. Indeed, it may be a legitimate case for using wide strings. But, if you are planning to add some configuration or a log file in future, please consider converting the whole thing to narrow strings. That would be future-proof.</p>
	</li>

	<li>
		<h3 id="faq.def.unicode"><a class="aId" href="#faq.def.unicode">Q: Why do you turn on the <code>UNICODE</code> define, if you do not intend to use Windows’ <code>LPTSTR</code>/<code>TCHAR</code>/etc macros?</a></h3>
		<p>A: This is an additional safety mechanism against plugging a UTF-8 <code>char*</code> string into ANSI-expecting function variants of Windows API. We want it to generate a compiler error. It is the same kind of a hard-to-find bug as passing an <code>argv[]</code> string to <code>fopen()</code> on Windows: it assumes that the user will never pass non-current-codepage filenames. You will be unlikely to find this kind of a bug by manual testing, unless your testers are trained to supply Chinese file names occasionally, and yet it is a broken program logic. Thanks to the <code>UNICODE</code> define, you get a compiler error for that.</p>
	</li>

	<li>
		<h3 id="faq.naive"><a class="aId" href="#faq.naive">Q: Isn’t it quite naïve to think that Microsoft will stop using widechars one day?</a></h3>
		<p>A: Let’s first see when they start supporting <code>CP_UTF8</code> as a valid codepage. This should not be very hard to do. Then, we see no reason why any Windows developer would continue using the widechar APIs. Also, adding support for <code>CP_UTF8</code> would ‘unbreak’ some of existing unicode-broken programs and libraries.</p>

		<p>Some say that adding <code>CP_UTF8</code> support would <em>break</em> existing applications that use the ANSI API, and that this was supposedly the reason why Microsoft had to resort to creating the wide string API. This is not true. Even some popular ANSI encodings are variable length (Shift JIS, for example), so no correct code would become broken. The reason Microsoft chose UCS-2 is purely historical. Back then UTF-8 hasn’t yet existed, Unicode was believed to be ‘just a wider ASCII’, and it was cosidered important to use a fixed-width encoding.</p>
	</li>

	<li>
		<h3 id="faq.boms"><a class="aId" href="#faq.boms">Q: What do you think about Byte Order Marks?</a></h3>
		<p>A: According to the Unicode Standard (v6.2, p.30): <q>Use of a BOM is neither required nor recommended for UTF-8</q>.</p>
		<p>Byte order issues are yet another reason to avoid UTF-16. UTF-8 has no endianness issues, and the UTF-8 BOM exists only to manifest that this is a UTF-8 stream. If UTF-8 remains the only popular encoding (as it already is in the internet world), the BOM becomes redundant. In practice, most UTF-8 text files omit BOMs today.</p>
		<p>Using BOMs would require all existing code to be aware of them, even in simple scenarios as file concatenation. This is unacceptable.</p>
	</li>

	<li>
		<h3 id="faq.crlf"><a class="aId" href="#faq.crlf">Q: What do you think about line endings?</a></h3>
		<p>A: Always use <code>\n (0x0a)</code> line endings, even on Windows. Files should be read and written in binary mode, which guarantees interoperability—a program will always give the same output on any system. Since the C and C++ standards use <code>\n</code> as in-memory line endings, this will cause all files to be written in the POSIX convention. It may cause trouble when the file is opened in Notepad on Windows; however, any decent text editor understands such line endings.</p>
		<p>We also prefer SI units, the <a href="https://en.wikipedia.org/?title=ISO_8601" rel="nofollow">ISO-8601</a> date format, and floating point to the <a href="https://en.wikipedia.org/wiki/Decimal_mark#Countries_using_Arabic_numerals_with_decimal_comma" rel="nofollow">floating comma</a>.</p>
	</li>

	<li>
		<h3 id="faq.uni.perf"><a class="aId" href="#faq.uni.perf">Q: But what about performance of text processing algorithms, byte alignment, etc?</a></h3>
		<p>A: Is it really better with UTF-16? Maybe so. ICU uses UTF-16 for historical reasons, thus it is quite hard to measure. However, most of the times strings are treated as cookies, not sorted or reversed every second use. A denser encoding is then favorable for performance.</p>
	</li>

	<li>
		<h3 id="faq.utf16.fault"><a class="aId" href="#faq.utf16.fault">Q: Is it really a fault of UTF-16 that people misuse it, assuming that it is 16 bits per character?</a></h3>
		<p>A: Not really. But yes, safety is an important feature of every design, and encodings are no exception.</p>
	</li>

	<li>
		<h3 id="faq.confuse"><a class="aId" href="#faq.confuse">Q: If <code>std::string</code> means UTF-8, wouldn’t that get confused with code that stores plain text in <code>std::string</code>s?</a></h3>
		<p>A: There is no such thing as plain text. There is no reason for storing codepage-ANSI or ASCII-only text in a class named ‘string’.</p>
	</li>

	<li>
		<h3 id="faq.cvt.perf"><a class="aId" href="#faq.cvt.perf">Q: Won’t the conversions between UTF-8 and UTF-16 when passing strings to Windows slow down my application?</a></h3>

		<p>A: First, you will do <em>some</em> conversion either way. It’s either when calling the system, or when interacting with the rest of the world, e.g. when sending a text string over TCP. Also, those of OS APIs which accept strings often perform tasks which are inherently slow, such as UI or file system operations. If your interaction with the system APIs dominate your application, here is a little experiment.</p>

		<p>One typical use of the OS APIs is to open files. This function executes in (184 ± 3)μs on my machine:</p>

		<pre><code>void f(const wchar_t* name)
{
    HANDLE f = CreateFile(name, GENERIC_WRITE, FILE_SHARE_READ, 0, CREATE_ALWAYS, 0, 0);
    DWORD written;
    WriteFile(f, &quot;Hello world!\n&quot;, 13, &amp;written, 0);
    CloseHandle(f);
}</code></pre>

		<p>While this runs in (186 ± 0.7)μs:</p>

		<pre><code>void f(const char* name)
{
    HANDLE f = CreateFile(widen(name).c_str(), GENERIC_WRITE, FILE_SHARE_READ, 0, CREATE_ALWAYS, 0, 0);
    DWORD written;
    WriteFile(f, &quot;Hello world!\n&quot;, 13, &amp;written, 0);
    CloseHandle(f);
}</code></pre>

		<p>(Run with <code>name=&quot;D:\\a\\test\\subdir\\subsubdir\\this is the sub dir\\a.txt&quot;</code> in both cases. It was averaged over 5 runs. We used an optimized <code>widen</code> that relies on <code>std::string</code> contiguous storage guarantee given by C++11.)</p>

		<p>This is just (1 ± 2)% overhead. Also, <code>MultiByteToWideChar</code> is not the fastest UTF-8↔UTF-16 conversion function.</p>
	</li>

	<li>
		<h3 id="faq.literal"><a class="aId" href="#faq.literal">Q: How do I write UTF-8 string literal in my C++ code?</a></h3>

		<p>A: If you internationalize your software then all non-ASCII strings will be loaded from an external translation database, so it is not a problem.</p>

		<p>If you still want to embed a special character you can do it as follows. In C++11 you can do it as:</p>

		<p class="display"><code>u8&quot;∃y ∀x ¬(x ≺ y)&quot;</code></p>

		<p>With compilers that do not support ‘u8’ you can hard-code the UTF-8 code units as follows:</p>

		<p class="display"><code>&quot;\xE2\x88\x83y \xE2\x88\x80x \xC2\xAC(x \xE2\x89\xBA y)&quot;</code></p>

		<p>However the most straightforward way is to just write the string as-is and save the source file encoded in UTF-8:</p>

		<p class="display"><code>&quot;∃y ∀x ¬(x ≺ y)&quot;</code></p>

		<p>Unfortunately, MSVC converts it to some ANSI codepage, corrupting the string. To work around this, save the file in UTF-8 <em>without BOM</em>. MSVC will assume that it is in the correct codepage and will not touch your strings. However, it renders it impossible to use Unicode identifiers and wide string literals (that you will not be using anyway).</p>
	</li>

	<li>
		<h3 id="faq.convert"><a class="aId" href="#faq.convert">Q: I have a complex large char-based Windows application. What is the easiest way to make it Unicode-aware?</a></h3>
		<p>Keep the chars. Define <code>UNICODE</code> and <code>_UNICODE</code> to get compiler errors where <code>narrow()</code>/<code>widen()</code> should be used (this is done automatically by setting <strong>Use Unicode Character Set</strong> in Visual Studio project settings). Find all <code>fstream</code> and <code>fopen()</code> uses, and use wide overloads as described above. By now, you are almost done.</p>
		<p>If you use 3rd-party libraries that do not support Unicode, e.g. forwarding file name strings as-is to <code>fopen()</code>, you will have to work around with tools such as <code>GetShortPathName()</code> as shown above.</p>
	</li>

	<li>
		<h3 id="faq.python"><a class="aId" href="#faq.python">Q: What about Python? I heard they worked hard in v3.3 to support Unicode better.</a></h3>
		<p>A: Perhaps, they should have done less and the support would have been better. In the CPython v3.3 reference implementation, the internal string representation was changed. The UTF-16 was replaced by one of three possible encodings (ISO-8859-1, UCS-2 or UCS-4) depending on the actual string content. To add a single non-ASCII or non-BMP character, the entire string will often be implicitly converted to a different encoding. The internal encoding is transparent to the script. This design is meant to <em>optimize the performance of indexing operations</em> on Unicode code points. However, we argue that counting or indexing code points <a href="#myth.strlen">should not be important</a> for the majority of uses—compared, for instance, to grapheme clusters. To our knowledge, Python currently provides no support of the latter.</p>

		<p>Therefore, we oppose representation-agnostic handling of strings, in favor of representation-transparent API with a UTF-8 internal representation. Indexing operations would be counting code units rather than the code points, as they in fact did before the change. This would also simplify the implementation and also improve performance, e.g. in scripts dealing with the Web, which is already dominated by UTF-8 encoded text, thus making the Python programming language more applicable in the server-side world. One may argue about the safety of string-cutting operations by script programmers, but then again, the same argument is valid for splitting grapheme clusters. Even though Unicode is now fully supported, we believe that Python, as a modern tool with less historical burden to carry, must do better job in text handling.</p>

		<p>Other than that, JPython and IronPython continue to rely on the less fortunate encoding used by their hosting platforms (Java and .NET, respectively) and care must be taken to handle the surrogate pairs correctly there.</p>
	</li>

	<li>
		<h3 id="faq.ood"><a class="aId" href="#faq.ood">Q: But why <code>std::string</code>? Wouldn’t it be a better, object-oriented approach to have a UTF-8 aware string class?</a></h3>
		<p>A: Not every piece of code dealing with strings is actually involved in processing and validation of text. A file copy program which receives Unicode file names and passes them to file IO routines, would do just fine with a simple byte buffer. If you design a library that accepts strings, the simple, standard and lightweight <code>std::string</code> would do just fine. On the contrary, it would be a mistake to reinvent a new string class and force everyone through your peculiar interface. Of course, if one needs more than just passing strings around, he should then use appropriate text processing tools. However, such tools are better to be independent of the storage class used, in the spirit of the container/algorithm separation in the STL. In fact, <a href="http://www.gotw.ca/gotw/084.htm">some consider</a> <code>std::string</code> interface too bloated, as most of it was better to be moved out of the <code>std::string</code> class.</p>
	</li>
	<li>
		<h3 id="faq.whats.now"><a class="aId" href="#faq.whats.now">Q: I already use this approach and I want to make our vision come true. What can I do?</a></h3>
		<p>A: Spread the word.</p>
		<p>Review your code and see what library is most painful to use in portable Unicode-aware code. Open a bug report to the authors. If you are a C or C++ library author, use <code>char*</code> and <code>std::string</code> with UTF-8 implied, and refuse to support ANSI code pages—since they are inherently Unicode-broken.</p>
		<p>If you are a Microsoft employee, push for implementing support of the <code>CP_UTF8</code> as one of narrow API code pages.</p>
		<p>Further ideas:</p>
		<ul>
			<li>Create a repository for patches of commonly used 3rd-party libraries for UTF-8 support (e.g. PugiXML, LibTIFF, etc.) These are written in standard C and do not care about Windows.</li>
			<li>Create a link-time patch for standard library functions (e.g. <code>fopen()</code>) on Windows, which would do the proper parameter conversion. This can be done for <code>main()</code> and the global environment variables.</li>
		</ul>
	</li>
</ol>


<h2 id="about"><a class="aId" href="#about">About the authors</a></h2>

<p>This manifesto was written by <a href="http://meshnotes.com/9SrRpAypTr8e">Pavel Radzivilovsky</a>, <a href="http://stannum.co.il/about">Yakov Galka</a> and <a href="http://slavanov.com/">Slava Novgorodov</a>. It is a result of our experience and research of real-world Unicode issues and mistakes done by real-world programmers. Our goal here is to improve awareness of text issues and to inspire industry-wide changes to make Unicode-aware programming easier, ultimately improving the experience of users of those programs written by human engineers. Neither of us is involved in the Unicode consortium.</p>

<p>Special thanks to Glenn Linderman for providing information about Python, and to Markus Künne, Jelle Geerts and Lazy Rui for reporting bugs and typos in this document.</p>

<p>Much of the text was inspired by <a href="http://programmers.stackexchange.com/questions/102205/should-utf-16-be-considered-harmful">discussions on StackOverflow initiated by Artyom Beilis</a>, the author of Boost.Locale. Additional inspiration came from the development conventions at <a href="http://www.visionmap.com">VisionMap</a> and Michael Hartl’s <a href="http://tauday.com/tau-manifesto">tauday.org</a>.</p>


<h2 id="extern"><a class="aId" href="#extern">External links</a></h2>

<ul>
	<li><a href="http://www.unicode.org/">The Unicode Consortium</a> (The Unicode Standard, <a href="http://www.unicode.org/versions/Unicode6.2.0/UnicodeStandard-6.2.pdf">PDF</a>)</li>

	<li><a href="http://site.icu-project.org/">International Components for Unicode</a> (ICU)</li>

	<li><a href="http://www.joelonsoftware.com/articles/Unicode.html">Joel on Unicode</a>—‘The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets’</li>

	<li><a href="http://cppcms.sourceforge.net/boost_locale/html/">Boost.Locale</a>—high quality localization facilities in a C++ way.</li>

	<li><a href="http://programmers.stackexchange.com/questions/102205/should-utf-16-be-considered-harmful">Should UTF-16 be considered harmful</a> on StackOverflow, started by Artyom Beilis.</li>

	<li><a href="https://dev.twitter.com/docs/counting-characters">How twitter counts characters</a></li>
</ul>

<h2 id="donate"><a class="aId" href="#donate">Feedback</a></h2>

<p>You can leave comments/feedback on the Facebook <a href="https://www.facebook.com/UTF8Everywhere">UTF-8 Everywhere</a> page. Your help and feedback are much appreciated.</p>

<p style="text-align:center">
<img src="data/utf8donate.png" alt="1UTF8gQmvChQ4MwUHT6XmydjUt9TsuDRn" /><br/>
Bitcoin donate to: 1UTF8gQmvChQ4MwUHT6XmydjUt9TsuDRn<br/>
The cash will be used for research and promotion.
</p>

<div id="g-plusone"></div>

<table class="layoutTable" style="border-top:1px solid silver; border-bottom:1px solid silver; padding:1ex 0ex; margin:1.5em 0 0.5em 0"><tr>
<td><a href="http://validator.w3.org/check?uri=referer"><img src="http://www.w3.org/Icons/valid-xhtml10" alt="Valid XHTML 1.0 Strict"/></a> <a href="http://jigsaw.w3.org/css-validator/check/referer"><img src="http://jigsaw.w3.org/css-validator/images/vcss" alt="Valid CSS!"/></a></td>
<td style="text-align:right">Last modified: <?php echo date("Y-m-d", getlastmod()); ?></td>
</tr></table>
</div>

<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-31306173-1', 'auto');
ga('send', 'pageview');
</script>


<script type="text/javascript">
gapi.plusone.render("g-plusone", { "size":"standard", "count":"true", "href":"http://utf8everywhere.org/", "annotation":"inline" });
</script>

</body>
</html>
